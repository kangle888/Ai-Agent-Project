import os
import logging
from dotenv import load_dotenv
from langchain.chat_models import init_chat_model
from langchain_core.exceptions import LangChainException

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


def init_llm_client():
    api_key = os.getenv("DEEPSEEK_API_KEY")
    if not api_key:
        raise LangChainException("DEEPSEEK_API_KEY not set")

    llm = init_chat_model(
        model="deepseek-v3.2",
        model_provider="openai",  # ğŸ‘ˆ å…¼å®¹æ¨¡å¼
        api_key=api_key,
        base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
        temperature=0.7,
        max_tokens=1000,
    )

    return llm


def main():
    try:
        load_dotenv()

        llm = init_llm_client()
        logger.info("LLMå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸï¼")

        # æ™®é€šè°ƒç”¨
        response = llm.invoke("ä½ æ˜¯è°ï¼Ÿ")
        print("å›ç­”ï¼š", response.content)

        # æµå¼è°ƒç”¨
        print("\n=================== æµå¼è¾“å‡º ===================")
        for chunk in llm.stream("ä»‹ç»ä¸‹LangChainï¼Œ300å­—ä»¥å†…"):
            print(chunk.content, end="")

    except Exception as e:
        logger.error(e)


if __name__ == "__main__":
    main()